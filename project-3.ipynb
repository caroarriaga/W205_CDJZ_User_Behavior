{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "## Understanding user behavior\n",
    "\n",
    "The game company we work for has two events that we want to track: buy a sword and join guild. Each of them has metadata characteristic of such events.\n",
    "\n",
    "## Our Business Questions\n",
    "\n",
    "Given the influence the business question has on the design on the pipeline, we chose to begin our presentation with the focus on our business question. Our questions focused on the trends of user behavior and internet host providers. To have a more basic and affordable pipeline, we chose to use daily batch processing of our events. The rationale behind this is we would not expect strong deviations in host providers or user events from day to day.\n",
    "\n",
    "## Pipeline description\n",
    "\n",
    "Before describing our pipeline, those who wish to replicate the pipeline should do so by running the steps described in the terminal and not a Jupyter notebook.\n",
    "\n",
    "1. **Sourcing:** Events are sourced to our system through the use of two python files \"game_api_with_extended_json_events\" and \"event_generator\" run through Flask. The game_api file produces two main logs to kafka - purchasing an item or joining a guild. These are the main two actions a user can take in the game. The event_generator takes the universe of use cases and serves as a proxy for daily user events. The file allows us the flexibility to create as many events as we desire. The events are as flat as possible to reduce the amount of data munging steps.\n",
    "\n",
    "2. **Ingesting:** Events are ingested through kafka where we have created a topic called \"events.\" Given the event structure and the fact that the \"event_generator\" file represents the universe of events, notifications were not needed and we only utilized one partition. The topic did not require further utilization of brokers.\n",
    "\n",
    "3. **Storing:** Once all events have been created, we will read the associated kafka topic (events) and  use the \"filtered_writes\" python file to unroll the json files and write the data to a parquet file. This parquet file will be the starting point for the analysis needed to answer the business questions. \n",
    "\n",
    "4. **Analysis**: Analysis begins by utilizing pyspark, we also need to use NumPy, Pandas and SparkSQL to sufficiently prepare the data to answer the business questions.\n",
    "\n",
    "\n",
    "# Creating a pipeline\n",
    "\n",
    "A docker container will manage all the services required in each step of the process via Zookeeper.\n",
    "To capture the information we want to analyze later, we propose the following data ingestion:\n",
    "\n",
    "YML Requirement: Zookeeper\n",
    "\n",
    "```YML\n",
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```\n",
    "\n",
    "## Instrumpent the API server to log events to Kafka\n",
    "\n",
    "In this first step, the user interacts with our mobile app. The mobile app maks calls to the web service and the API server handles the requests: buy a sword or join a guild. Then the request is logged into Kafka.\n",
    "\n",
    "YML Requirement: kafka\n",
    "\n",
    "- depends on Zookeeper\n",
    "- gathers all logs existent\n",
    "- exposes ip address to connect with live data\n",
    "\n",
    "```YML\n",
    "kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\" \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a topic in kafka\n",
    "\n",
    "In docker we can create an event in kafka were all the events will be registered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Zookeeper and Kafka we can test our API server loging events to Kafka.\n",
    "\n",
    "Spin up the cluster:\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Create a topic **events**\n",
    "\n",
    "```python\n",
    "docker-compose exec kafka \\\n",
    "   kafka-topics \\\n",
    "     --create \\\n",
    "     --topic events \\\n",
    "     --partitions 1 \\\n",
    "     --replication-factor 1 \\\n",
    "     --if-not-exists \\\n",
    "     --zookeeper zookeeper:32181\n",
    "```\n",
    "After running in the CL, it should show the topic events has been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"carolina_daniel_jonathan_ziwei_default\" with the default driver\n",
      "Creating carolina_daniel_jonathan_ziwei_zookeeper_1 ... \n",
      "Creating carolina_daniel_jonathan_ziwei_mids_1      ... \n",
      "Creating carolina_daniel_jonathan_ziwei_cloudera_1  ... \n",
      "\u001b[3BCreating carolina_daniel_jonathan_ziwei_kafka_1     ... mdone\u001b[0m\u001b[3A\u001b[2K\n",
      "\u001b[2BCreating carolina_daniel_jonathan_ziwei_spark_1     ... mdone\u001b[0m\n",
      "\u001b[1Bting carolina_daniel_jonathan_ziwei_spark_1     ... \u001b[32mdone\u001b[0m\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "! docker-compose up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created topic events.\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec kafka \\\n",
    "   kafka-topics \\\n",
    "     --create \\\n",
    "     --topic events \\\n",
    "     --partitions 1 \\\n",
    "     --replication-factor 1 \\\n",
    "     --if-not-exists \\\n",
    "     --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API POST requests\n",
    "\n",
    "The game company must redirect the events to specific HTTPS (API endpoint) for example:\n",
    "\n",
    "- POST/purchase/item_name\n",
    "- POST/join_a_guild\n",
    "\n",
    "Examples:\n",
    "\n",
    "| Items | Event | API endpoint |\n",
    "| --- | --- | --- | \n",
    "| sword | Purchase | POST/puchase/sword |\n",
    "| shield | Purchase | POST/puchase/shield |\n",
    "| helmet | Purchase | POST/puchase/helmet |\n",
    "| knife | Purchase | POST/puchase/knife |\n",
    "| gauntlet | Purchase | POST/puchase/gauntlet |\n",
    "| NA | Join a guild | POST/ join_a_guild |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use GET to retrieve specific call using\n",
    "\n",
    "GET/join_a_guild\n",
    "\n",
    "For our project, we will use the suggested endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loging events\n",
    "\n",
    "We are proposing using Flask to route the event calls and log them.\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "producer = KafkaProducer(bootstrap_servers='kafka:29092')\n",
    "\n",
    "\n",
    "def log_to_kafka(topic, event):\n",
    "    event.update(request.headers)\n",
    "    producer.send(topic, json.dumps(event).encode())\n",
    "    \n",
    "@app.route(\"/purchase/<item_name>\")\n",
    "def purchase_a_sword(item_name):\n",
    "    purchase_event = {'event_type': '{} purchased'.format(item_name)}\n",
    "    log_to_kafka('events', purchase_event)\n",
    "    return \"{} Purchased!\\n\".format(item_name)\n",
    "\n",
    "@app.route(\"/join_a_guild\")\n",
    "def join_guild():\n",
    "    join_guild_event = {'event_type': 'join_guild'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined guild.\\n\"\n",
    "```\n",
    "\n",
    "This file will be included in the same folder under as a python file under the name game_api_with_extended_json_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Flask\n",
    "To run t use (use another terminal)\n",
    "\n",
    "```python\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-caroarriaga/game_api_with_extended_json_events.py flask run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"game_api_with_extended_json_events\"\n",
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    " ! docker-compose exec mids env FLASK_APP=/w205/project-3-caroarriaga/game_api_with_extended_json_events.py flask run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a script (event_generator.py) that hits the flask app with various quantities and combinations of event types, with randomly generated host names. This generated data will go through the pipeline and analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.18.5)\n",
      "Traceback (most recent call last):\n",
      "  File \"event_generator.py\", line 72, in <module>\n",
      "    output = subprocess.check_output(split_cmd)\n",
      "  File \"/opt/conda/lib/python3.7/subprocess.py\", line 411, in check_output\n",
      "    **kwargs).stdout\n",
      "  File \"/opt/conda/lib/python3.7/subprocess.py\", line 512, in run\n",
      "    output=stdout, stderr=stderr)\n",
      "subprocess.CalledProcessError: Command '['docker-compose', 'exec', 'mids', 'ab', '-n', '2', '-c', '1', '-T', '-p', '-H', 'Host: user46.earthlink.com', 'http://localhost:5000/purchase/shield']' returned non-zero exit status 111.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!python event_generator.py 5 #will run 5 separate ab commands, each with an inverse-logarithmic number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from Kafka\n",
    "\n",
    "We can read all the logged events using from the \"events\" topic:\n",
    "\n",
    "```python\n",
    "docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t events -o beginning -e\"\n",
    "```\n",
    "\n",
    "### Example of output\n",
    "\n",
    "After using flask to capture data the API requests and processed them using Flaks this is how the log should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR\u001b[0m: Aborting.\n"
     ]
    }
   ],
   "source": [
    "! docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t events -o 25 -c 20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating events and land in HDFS\n",
    "\n",
    "filtered_writes.py parses and separates purhase events and join guild events into two separate files in HDFS, ready to be queried and analyzed.\n",
    "\n",
    "#### python file that filters and writes events into files by type\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Extract events from kafka and write them to hdfs\n",
    "\"\"\"\n",
    "import json\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if 'purchase' in event['event_type']:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_join_guild(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type']=='join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"main\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"ExtractEventsJob\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    raw_events = spark \\\n",
    "        .read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"endingOffsets\", \"latest\") \\\n",
    "        .load()\n",
    "\n",
    "    purchase_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_purchase('raw'))\n",
    "\n",
    "    extracted_purchase_events = purchase_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()\n",
    "    extracted_purchase_events.printSchema()\n",
    "    extracted_purchase_events.show()\n",
    "\n",
    "    extracted_purchase_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/purchases')\n",
    "\n",
    "    join_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_join_guild('raw'))\n",
    "\n",
    "    extracted_join_events = join_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()\n",
    "    extracted_join_events.printSchema()\n",
    "    extracted_join_events.show()\n",
    "\n",
    "    extracted_join_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/join_guild')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "#### Run it \n",
    "\n",
    "\n",
    "```\n",
    "docker-compose exec spark spark-submit /w205/project-3-caroarriaga/filtered_writes.py\n",
    "```\n",
    "\n",
    "Check output \n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "Should look like this, files join_guild and purchases were created\n",
    "```\n",
    "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
    "drwx-wx-wx   - root   supergroup          0 2021-04-03 22:24 /tmp/hive\n",
    "drwxr-xr-x   - root   supergroup          0 2021-04-05 00:09 /tmp/join_guild\n",
    "drwxr-xr-x   - root   supergroup          0 2021-04-05 00:09 /tmp/purchases\n",
    "```\n",
    "\n",
    "These files are ready for query in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-8c5a314bef77>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-8c5a314bef77>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    docker-compose exec spark spark-submit /w205/carolina_daniel_jonathan_ziwei/filtered_writes.py\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "docker-compose exec spark spark-submit /w205/project-3-caroarriaga/filtered_writes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!docker-compose exec spark spark-submit /w205/project-3-caroarriaga/filtered_writes.py\n",
    "#this needs to be opened in another terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - root   supergroup          0 2021-04-07 01:19 /tmp/hive\n",
      "drwxr-xr-x   - root   supergroup          0 2021-04-07 01:23 /tmp/join_guild\n",
      "drwxr-xr-x   - root   supergroup          0 2021-04-07 01:23 /tmp/purchases\n"
     ]
    }
   ],
   "source": [
    "!docker-compose exec cloudera hadoop fs -ls /tmp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Pyspark to analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, spin up a Pyspark cluster\n",
    "\n",
    "```python\n",
    "docker-compose exec spark pyspark\n",
    "```\n",
    "\n",
    "import the necessary tools\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "```\n",
    "transform the parquet file to spark dataframes\n",
    "\n",
    "```python\n",
    "purchases = spark.read.parquet('/tmp/purchases')\n",
    "join = spark.read.parquet('/tmp/join_guild')\n",
    "```\n",
    "redefine the host provider list to answer the second business question. the host provider list represent the universe of internet service providers\n",
    "\n",
    "```python\n",
    "host_provider_list = ['comcast','att','google','frontier','cox','earthlink']\n",
    "```\n",
    "\n",
    "Our first business question is what are the most popular events across our user base?\n",
    "\n",
    "```python\n",
    "purchases.groupBy('event_type').count().sort(col(\"count\").desc()).show()\n",
    "```\n",
    "\n",
    "We see that purchasing a knife is by far the most popular event, with 49 instances. The second is purchasing a helmet with 30 instances and purchasing a shield at 29 instances.\n",
    "\n",
    "This insight allows us to improve gameplay by creating more knife varieties and using future data analysis to inform strategies to encourage an increase of user events around less popular event types (e.g. purchasing gauntlets)\n",
    "\n",
    "\n",
    "To answer the next business question, we will need to transform the pyspark data frame to a Pandas data frame\n",
    "```python\n",
    "purchases_2 = purchases.toPandas()\n",
    "```\n",
    "\n",
    "Our next business question builds from the initial question and ask what internet host providers do our users utilize and what events are most popular on those hosts?\n",
    "However, the internet host provider information needs to be parsed since we cannot clearly aggregate the data because there is a one to one relationship between users and host.\n",
    "\n",
    "We are going to define a matcher function to parse through the variable \"host_provider_list\"\n",
    "\n",
    "```python\n",
    "def matcher(x):\n",
    "        for i in host_provider_list:\n",
    "            if i in x:\n",
    "                return i\n",
    "        else:\n",
    "            return np.nan \n",
    "```\n",
    "\n",
    "We will them apply the matcher function to our Pandas data frame and create a new column called \"Host\" to represent the parsed host name\n",
    "\n",
    "```python\n",
    "purchases_2['Host'] = purchases_2['Host'].apply(matcher)\n",
    "```\n",
    "We will then group the data frame by host names and event types and show the data frame\n",
    "\n",
    "```python\n",
    "purchases_3 = purchases_2.groupby(['Host', 'event_type']).count()[['Accept']]\n",
    "\n",
    "purchases_3.rename(columns={\"Accept\":\"Count\"})\n",
    "```\n",
    "\n",
    "We can see our users mainly use cox and earthlink as their internet hosts, with both providers hosting 38 people. Additionally, we see that most of our knife purchases come from users hosted on earthlink and most of our helmets are purchased from users using cox. \n",
    "\n",
    "The importance of this discovery is that we can focus on developing relationships with local providers to understand any potential service issues that might impact gameplay. We can explore the option of prioritizing traffic from earthlink and cox since they represent a significant amount of our users. Further, we can observe host trends as a proxy of user behavior. For example, if most of our users move from earthlink to att since the service is more affordability, and att has reliability issues, we may see a decrease in user purchases. This external impact would mean we could save time and effort on understanding if this issue was platform driven. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
